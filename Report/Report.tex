\documentclass[a4paper,12pt,table]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage[table,xcdraw]{xcolor}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{blindtext}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}


\renewcommand{\arraystretch}{2.5}

\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\rhead{
}
\fancyfoot[R]{\thepage}
\graphicspath{ {C:/Users/sarya/Desktop/Semester 4/ISM/Report} }
\begin{document}

\begin{titlepage}
   \begin{center}
       \vspace*{6cm}

       \textbf{{\Huge Independent Study Module Report}}

	\vspace{5cm}

 {\Large An Investigation of Reinforcement Learning Algorithms for Mastering the Game of Santorini}

       \vspace{0.5cm}

       \vspace{1.5cm}


       \vfill



       \textbf{Aryan Sarswat -  A0200521E \\
		Lim Wei Liang - A0205466E \\
		Roy Chua Dian Lun - A0199930N \\ }

       \vspace{0.8cm}

      University Scholars Programme\\
       National University of Singapore\\

   \end{center}
\end{titlepage}


\tableofcontents
\newpage


\section{Introduction}

\section{Santorini}

\subsection{Game Rules}
Santorini is a 2-player board game on a 5x5 grid, with 3 distinct pieces: building blocks, the worker and the dome. Each turn a player will move his worker to an adjacent tile, then place a building block on a tile adjacent to the moved worker. The building blocks can be stacked up to 3 levels. The worker can only traverse to an adjacent tile that is within 1 level of the worker’s current tile (i.e. if the worker is on level 1, he can only move to an adjacent tile of level 0, 1 or 2). When there are 3 building blocks on a tile, a dome can be placed on top of the blocks to prevent another player from reaching level 3. The worker has to traverse from the ground level (level 0) to level 3 in order to win the game. Another way to win is to force the opposing worker to have no adjacent tiles to move to. 

\subsection{Game Code}
An Object-Oriented Programming (OOP) approach was employed to simulate the board-game, as it was deemed as the most “user-friendly” approach, allowing ease of code review and overall game design. The main objects that were instantiated were: 1) Worker, 2) Square, 3) Board, and 4) Player. All of the game’s code was done in Python 3.9.

\begin{enumerate}
    \item \underline{Worker Class}
    \newline
    The Worker class is a base class, representing a Worker object on the Board. The worker has two pieces of information stored: the player it belongs to, and its position on the board (represented by a tuple of x-y coordinates). It has a method for movement about the board. 
    \item \underline{Square Class}
    \newline
    The Square class represents a “square” on the board, having immutable x-y coordinates (represented as a tuple) as an attribute. It possesses two pieces of information: the level of buildings (represented as an integer ranging from 0 to 4), and whether a Worker is present (represented by either a Worker object or None).
    \item \underline{Board Class}
    \newline
    The Board class represents the entire state of the board game, which is the main object used for interaction with the players. The space of the board (a 5x5 grid) is represented as a list of 25 Square objects. The Board class has methods for checking terminal states and validity of player actions. 
    \item \underline{Player Class}
    \newline
    The Player class is a super class for all agents and human players. It possesses information about its Worker locations, as well as two methods: one to place its 2 workers at the start of the game, and one to simulate an action (movement of a worker piece, followed by placement of building). \par

\end{enumerate}
A simulation of the Board game will start with the creation of a Board object and 2 Player objects (either human or AI), and initialize worker placement depending on the Player object. The players (starting from Player A) will take turns doing an action until a terminal state is reached, where either player wins, of which the script will terminate for a single simulation.



\section{Methods employed to create AI Agent}

\subsection{Linear Value Function Approximation}

\subsection{Neural Networks (Value Function Approximation)}
Using Linear Value Function Approximation with minimax is an extremely effective process. In games where the branching factor is not very large, minimax can lead to optimal gameplay by the AI agent. However, due to the relatively large branching factor of Santorini (128 moves per turn), a reasonably fast AI agent can play by looking up to 4 states maximally. This severely handicaps the performance of the AI. This is one of the main reasons to attempt to use a Neural Network to predict the value of a state instead. \par

Neural Networks are essentially seen as universal function approximators, which can be used to mimic any function. Thus by using the minimax algorithm we can find the true value of a state and use that as our training data. This training data will be fed into the neural network which will adjust its weights and bias to be able to create a function which can accurately determine how strong a certain state is. This will increase the speed at which our agent can predict the value of a state as the minimax algorithm determines the value of the state by back propagating the value from a terminal state, which is why there is no need to look ahead further states as they are already accounted for. This aspect of Neural Networks as function approximators significantly speed up the “thinking-time” of our agent. \par

One Hot Encoding was employed to turn the board state into a flattened array of size [1,325] containing only ones and zeros. This was used to ensure that the height of the building is not given priority due to its large numbers (1-4). \par

\paragraph{ANN Architecture}
\begin{enumerate}
    \item \underline{Input}
    \newline
    Consists of a 1,325 layer of Neurons as each board state contains 25 squares and there are 13 unique states thus a hot encoded flattened matrix would have 325 ones and zeros.
    \item \underline{Neuron Layers}
    \newline
    This is actually a hyperparameter which we can change, for our experiments there was one hidden layer consisting of 256 input neurons and 64 output neurons. This layer was chosen arbitrarily.
    \item \underline{Output}
    \newline
    The output consists of a tensor with one value. This value is between [-1,1] inclusively. The higher its value is the better the state is for Player 1 and the lower it is the better the state is for Player 2. A value of 0 implies that the state is equally strong for both players.
    \item \underline{Activation Functions}
    \newline
    The Rectified Linear unit (ReLU) activation function was employed to linearize the function and reduce its computational complexity. The Tanh function was also used in the output layer to ensure that the output was between [-1,1]. This not only squashed the results into this range but also did it proportionally. Thus if the value of a state is extremely large it would be reduced to 1 which can be interpreted as a completely winning state for Player 1.
    \item \underline{The Loss Function}
    \newline
    The Mean Squared Error loss was employed, this ensured that the state values did not explode as it heavily punishes large differences between the true value and the value predicted by the neural network.
\end{enumerate}

\begin{center}
    \textbf{Picture of ANN Architecture}
\end{center}

\subsection{Neural Networks with Convolutional Layers (Value Function Approximation)}

\underline{Description}
\newline
In order to make a value function approximator that is capable of converting board information to a state-value, a Convolutional Neural Network (CNN) was used to attempt to produce an accurate state-value given a board state. A CNN is capable of extracting relevant information from a multidimensional array.  It is a type of neural network that is commonly employed in fields of Computer Vision, such as feature detection of images. One common example is facial recognition technology, where a CNN is trained and employed to detect facial features.  \par

\underline{Rationale}
\newline
The rationale of employing a CNN in the determination of the state-value of a board, is mainly due to the capability of a CNN in retaining the 2-dimensional structure of an “image” of a board, whilst generating and preserving 2D spatial features that a normal ANN is not capable of. This is because a board is fundamentally 2-dimensional in nature, and the player is traversing in a 2D-board space (3D if the buildings are included). Therefore, features of the Santorini board, like the grouping of high-level buildings, or the proximity of a player to a building, is better extracted by a CNN rather than a simple 1D neural network structure. The feature extraction capability of a CNN is pivotal in determining how much “value” the board is to a player. \par

\underline{CNN Architecture}
The neural network was split into two distinct sections, the CNN layers (for feature extraction) and the ANN layers (for state-value determination).  \par


\begin{figure}[h!]
    \begin{center}
        \includegraphics[scale=0.4]{CNN_Archi.png}
        \caption{CNN architecture for first value function approximator}
        \label{fig:}
    \end{center}
\end{figure}

\begin{enumerate}
    \item \underline{Input}
    \newline
    From the board, two sources of information were determined to be essential for the neural network to output an accurate state-value: worker position and building level. Hence, the input layer had 2 input features. The board was converted into 2 layers. Each layer is a 2-dimensional 5x5 array, representing the board structure. The first layer consisted of building data, where the possible building levels of 0 to 4 were min-max normalized (0 representing ground level, 0.25 representing level 1 etc). The second later consisted of worker data, where the player’s worker was denoted as 1, and the opponent’s worker was denoted as -1. 
    \item \underline{Output}
    \newline
    The output was a tensor with a single value that represented the state-value of the board, with the maximum value of 1 being favourable for the player (aka a winning state) and the minimum value of -1 being not favourable.
    \item \underline{CNN Layers}
    \newline
    The first CNN layer had an input feature value of 2, representing the building levels and worker positions. It outputted 16 feature channels, with a default stride value of 1. The second CNN layer had similar hyperparameters to the first layer, the only difference being the input features to be 16, corresponding to the out channels of the first layer.
    \item \underline{Batch Normalization}
    \newline
    Batch normalization was implemented after every convolutional layer to coordinate the updating of weights at the end of every batch.
    \item \underline{Pooling}
    \newline
    Pooling was not implemented in the CNN as the value function should retain as much information about the board as possible. 
    \item \underline{Kernal Size}
    \newline
    A kernel size of 3 was adopted to capture the possible features of the board, as the    
    worker could move in the adjacent 8 squares, hence the features that are extracted 
    will be more relevant for the player’s worker.     
    \item \underline{Padding}
    \newline
    Padding with value of 1 was used to capture the information of the board corners more accurately.
    \item \underline{Activation Function}
    \newline
    The Rectified Linear unit (ReLU) activation function was adopted for both the CNN and ANN layers (bar the second ANN layer), for less computational complexity and its linear behaviour.
\end{enumerate}

\underline{Training of the first CNN Model}
\newline
The first CNN model was trained on a random agent (who made random worker movements and worker placements) for 5000 iterations. Each iteration represents a complete game of Santorini. For each state, the CNN agent will generate all possible next states, evaluate the states through the CNN, and output the state-value for each possible state. Based on an epsilon-greedy policy, it will then either make a random move, or make a move based on the state with the highest state-value. The state-value of the next state (either the random state or the state with the highest state-value) is saved in an array for backpropagation at the end of the iteration. As the reward given to the agent is only known at the end of the iteration, there is no update of weights to the CNN until the end of one iteration.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{Model Trainied}
    \ForEach(){Episode}{
        \While{game is not terminal}{
            Generate all possible states\;
            Generate all possible states-values for all states\;
            r $\leftarrow$ generate random number\;
            \eIf{r $>$ $\epsilon$}{
             Make action with the highest state-value\;
             }{
             Make random action\;
            }
            Store state-value of action made\;
            }
            $\epsilon$ $\leftarrow$ $\epsilon$ $\times$ ($\epsilon$ - decay)\;
            R $\leftarrow$ 1 (Agent Win) or -1 (Agent Loss)\;
            Backpropagate loss based on R\;          
    }
    \caption{Training CNN Model}
\end{algorithm}

\subsection{Monte Carlo Tree Search}
The Monte Carlo Tree Search algorithm is a variant of the standard Monte Carlo method where a search tree is constructed creating a sort of policy network for an agent to follow. It relies on rollouts to be able to successfully determine the value of a state. This is a very popular algorithm used in many board games; It was successfully implemented by researchers at Deep-Mind to develop AlphaGo and AI which was able to reach grandmaster level ELO at the game of GO. Monte Carlo Tree search is executed by 4 key steps Selection, Expansion, Simulation and Backup. In one run of the algorithm first Selection occurs and if the node is a leaf node it is expanded, after which a number of games are simulated from that leaf node, finally the values obtained from simulating these games are back propagated up to the root node. These four steps will be elaborated upon in detail below:

\begin{enumerate}
    \item \textbf{Selection:} Beginning at the root node, select children node based on a heuristics which involve the visit count and value of the child state. For our experiment the Upper Confidence Bound. More specifically the UCB1 algorithm was used where along with the value of the state the number of visit of the child node and the parent node is used to determine the criteria for selecting a child node. This algorithm allows for the selection of nodes which have not been visited and thus promotes exploration. This algorithm also contains a hyperparameter $c$ which is known as the exploration parameter and it is usually determined empirically but we have used the theoretical value of $\sqrt{2}$.
    \[v_i + c\sqrt{\frac{\ln{N_p}}{n_i}}  \]
    \item \textbf{Expansion:} Once a leaf node is reached, the tree is expanded by adding children to the leaf node, these children are all the possible next states which may follow. This allows for the growth of the search tree. However if the leaf node is terminal it will not be expanded and in some cases one may choose not expand a leaf node completely.
    \item \textbf{Simulation:} From the selected node or from a node which has been expanded, a complete episode starting from that node is simulating until the state is terminal. After Which a reward is returned. This is also known as a rollout and the games are simulated with a certain policy; this policy could be a Linear Value Function, a random Policy or even a through a Neural Network which was trained under expert games. Other than the node selected for the simulation, none of the other states in the simulation are stored, this significantly reduces the memory requirements compared to other methods such as Q-tables.
    \item \textbf{Backup:} The return obtained from the simulated episode is the backed up the path taken to reach the leaf node or if it is the first node in the tree it used to initialize the value for the node. These values are then added up for each simulation in each node in a value sum variable, this will be an indication how good or bad the state is. This is one of the key steps of the MCTS algorithm as instead of using randomly generated episodes, where the goal is to explore as many states as possible, it enriches this data and even though lesser states are explored, the data contains more information than that of a randomly generated episode.
\end{enumerate}

The figure below visually depicts how the algorithm runs \par

\begin{figure}[h!]
    \begin{center}
        \includegraphics[scale=0.65]{MCTS.png}
        \caption{one iteration of MCTS algorithm}
        \label{fig:}
    \end{center}
\end{figure}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwIn{root}
    \KwOut{root}
    \KwResult{One Iteration of MCTS}
    \While{time/resource left}{
        \eIf{root is not expanded}{
            Expand Root\;
        }{
            $search\_path$ $\leftarrow [root]$\;
            $current\_node$ $\leftarrow$ root\;
            \While{$current\_node$ is expanded}{
                $current\_node$ $\leftarrow$ Select child from $current\_node$\;
                $search\_path$.append($current\_node$)\;
            }
            \For{Simulation $\in \{1,2,3,....., \text{Number of Simulations}\}$}{
                Reward $\leftarrow$ Rollout from $current\_node$\;
                Backpropagate Reward\;
            }
            \If{$Current\_node$ is not terminal}{
                Expand $current\_ node$\;
            }
        }
    }       
    \caption{Monte Carlo Tree Search}
\end{algorithm}

This algorithm continues to run until either a time limit is reached or some other computation resource is depleted such as memory. After the algorithm stops running the tree can be used as policy to be able to play games. For each node, the child node with the highest value can be selected, this works because if the visit count and the value of the state is high it implies during the simulation the tree had often selected this node, which means that it often wins in this state. \par

In addition to the above the previous tree can be improved upon by selecting a leaf node which has not terminated and setting that node as the new root node to construct another MCTS tree. This will allow for greater exploration and better approximation of the value of states.However due to our computational constraints we were unable to run a the MCTS algorithm for a significant upon of time, this is due to the fact that for the value of any node to converge we need a high number of simulation and this is not easily achievable on standard computers as they make take upto 4 to 5 days to complete the construction of one MCTS tree. Thus we developed a variation of the MCTS tree which considers the breadth of the tree instead of a depth. \par

The Algorithm is as follows: for each node, all the child nodes are selected one by one and for each of these node games are simulated to obtain a value of how good this state is, this value if normalized by the number of games simulated. After all the games have been simulated for all the child nodes, using the UCB1 algorithm a child node is selected which has the highest value, this child node then becomes the new root node. And for each iteration this repeatedly results in a higher depth explored. This algorithm significantly cuts down on the computational resources required, however this comes at a cost; since we are using the new child nodes as the root nodes for another tree, we are essentially doing a one step look ahead, the values are not backed up all the way to the root node, this reduces the chances of the value of the state to converge. Furthermore it constricts the number of nodes which explores as it only searches down the path with the highest UCB1 value. \par


\subsection{Combining the Above}


\section{Results and Dicussion}

text \par

\begin{table}[H]
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|cccccccccc}
    \hline
    \multicolumn{11}{|c|}{Win Rates}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\ \hline
                   & \multicolumn{1}{c|}{Random}                     & \multicolumn{1}{c|}{ANN}                        & \multicolumn{1}{c|}{CNN}                        & \multicolumn{1}{c|}{MCTS\_ANN}                  & \multicolumn{1}{c|}{MCTS\_CNN}                  & \multicolumn{1}{c|}{Combined}                   & \multicolumn{1}{c|}{Linear TreeStrap}             & \multicolumn{1}{c|}{Linear RootStrap}             & \multicolumn{1}{c|}{Linear Manual}             & \multicolumn{1}{c|}{Human}                      \\ \hline
    Random         & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-2}
    ANN            & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-3}
    CNN            & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{333333} } & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-4}
    MCTS\_ANN      & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } \\ \cline{1-5}
    MCTS\_CNN      & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-6}
    Combined       & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } & \cellcolor[HTML]{000000}{\color[HTML]{000000} } \\ \cline{1-7}
    Linear TreeStrap & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-8}
    Linear RootStrap & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-9}
    Linear Manual & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        & \cellcolor[HTML]{000000}                        \\ \cline{1-10}
    Human          & \multicolumn{1}{c|}{100\%}                      & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                           & \cellcolor[HTML]{000000}                        \\ \cline{1-10}
    \end{tabular}%
    }
    \caption{Win Rate of Each Agent}
    \label{table:}
\end{table}

\section{Conclusion}

\begin{thebibliography}{9}
    \bibitem{Reinforcement learning: An introduction} 
    Sutton, R.S., \& Barto, A.G. 
    \textit{Reinforcement learning: An introduction}. 
    Cambridge (Mass.): The MIT Press.
    
    \bibitem{Why Deep Neural Networks for Function Approximation} 
    Liang, Shiyu, and R. Srikant. 
    \textit{Why Deep Neural Networks for Function Approximation?}. 
    ArXiv:1610.04161 [Cs], Mar. 2017. arXiv.org, http://arxiv.org/abs/1610.04161.

\end{thebibliography}

\end{document}


